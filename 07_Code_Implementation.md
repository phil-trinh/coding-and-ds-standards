# 7 - Code Implementation
## Expected Outputs from this Stage
- [ ] Column Selects
- [ ] Spark Order of Operations
- [ ] Data Expectations 
- [ ] Schema Definition
- [ ] Update Docstrings & Column Descriptions
- [ ] Type Hints
- [ ] Unit Tests
- [ ] Debuggers and Data Preview
- [ ] PySpark Dataset Builds
- [ ] Incremental Builds


## Column Selects
In order to optimize processing and transformation, it is generally best to select only the columns or features that are
 needed. This limits the need to load data and minimizes the compute load throughout all the different transformations.
 Within PySpark, the select() function is used to select single, multiple, columns either by name, string regex, or
 index, all of which returns a new DataFrame with the selected columns.
```
# Using col() function
from pyspark.sql.functions import col
df.select(col("firstname"),col("lastname"))

# Using regular expression to select the similarly named columns
df.select(df.colRegex("`^.*name*`")).show()

# Using indices (in this case, index 2 to index 4) to select the same columns
df.select(df.columns[2:4])

# Pandas example
df["firstname"]                         # Direct name of column
df[["firstname", "lastname"]]           # List of direct names of columns
df.loc[:, ["firstname", "lastname"]]    # Using .loc to select both rows and columns by row index and column names
df.iloc[:, 0:2]                         # Using .iloc to select both rows and columns by index
```



## Spark Order of Operations
In general, Spark follows standard SQL logical order of operations, which is useful to keep in mind as transforms are
 layered together. For reference:

```
FROM, including JOINs
WHERE
GROUP BY
HAVING
WINDOW
SELECT
DISTINCT
UNION
ORDER BY
LIMIT
```

While these functions do not necessarily need to combined into a single query like in SQL, following the logic above in
 Spark (for example, joining relevant dataframes early on when possible or ordering the output later on when possible)
 can help optimize your code.



## Data Expectations
Data Expectation Checks are proactive requirements defined on input and output datasets that improve pipeline stability.
 Expectation checks help to catch assumption failures and build issues earlier without letting bad data propagate
 downstream.

### Applying Expectations
Data expectations can be applied to datasets as a whole or on columns in a dataset. Additional logic in a script is
 applied to check that certain defined conditions are true. If a certain condition is not met, the script should either
 make note of the failure and continue to run, or cause the build to fail and draw attention to the cause of the
 failure. 

### Considering Expectations
Expectation checks applied to input and output datasets will depend on dataset requirements. However, there are some
 common checks that can be applied in most cases:
- Primary key: Checks for nulls and uniqueness in the primary key column or across all columns that would make up a
  composite key.
- Column exists: Checks that a certain column exists in a dataset. This is helpful to place on input datasets where
  certain columns are required for logic. 
- Schema contains: Checks that a dataset contains a list of columns. 
- Has type: Checks if a column is of a certain data type.
- Greater than/ less than: Checks that the values in a certain column are greater than or less than a particular value.
  It can also check against values in other columns. 



## Update Schema Definition, Docstrings, & Column Descriptions
During the code implementation process, the final schema, docstrings, and column descriptions should be updated to reflect final code
 and data elements being used. In code documentation is only good if it's being kept up with the code it's describing.



## Provide Type Hints to All Inputs & Outputs
[Type Hinting – Real Python](https://realpython.com/lessons/type-hinting/)

Type hints indicate the type of value required or generated by inputs and outputs in functions. They improve the
 experience of function users as users will always understand the requirements to use the function, as well as what
 they can expect the function to return.

### Example and syntax breakdown
Below is an example of a function with type hints defined. Our function is called 'greet.' It has one argument, 'name,'
 which is indicated to be type 'str.' The syntax 'name: str' defines the name of the argument left of the colon and
 tells us that whatever value we pass to the function for the 'name' argument must be of type 'str' or the code will not
 run. Finally, we see that our function's output will be of type 'str,' indicated by the right-facing arrow syntax.

```
def greet(name: str) -> str:
    return "Hello, " + name
``` 



## Unit Tests
[Python Testing - Real Python](https://realpython.com/python-testing/)

### Types of Code Testing
There are two main types of testing: unit testing and integration testing. 
Unit tests ensure that individual units (e.g., functions) of code accomplish their intended purpose. 
Integration tests ensure that multiple units of code can run together (i.e., integrate) without error.

### Benefits of Unit Testing
Unit tests are vital to code implementation because they:
- Ensure that code logic generates expected results
- Ensure the code meets quality standards
- Provide additional documentation
- Provide examples of calling the function
- Demonstrate examples of possible use cases and results
- Ideally address not only the general use case, but also plausible edge cases (e.g., null input values)

### How to run unit tests
Unit tests consist of multiple parts:

- creating sample inputs
- generating actual outputs by calling the function being tested on those sample inputs
- comparing actual outputs with expected outputs
    - this is done by using the assert keyword
        - for example, assert actual_outputs == expected_outputs
    - if actual_outputs do not equal expected_outputs, Python will generate an error and stop running the script

### Example
Below is an example of a simple function add_one we want to test, contained in file example_script.py:
```
#-----------example of a function to be tested-----------#
 
# import libraries
import pandas as pd
import numpy as np
 
# create simple function, add_one, that we will test
def add_one(df, column):
    df[column] = df[column] + 1
    return df
```
Below is an example of a manual unit test test_add_one being performed on function add_one.
```
#-----------example of a manual unit test-----------#
 
# import libraries
import pandas as pd
import numpy as np
 
# import function to be tested
from example_script import add_one
 
# create sample inputs in the "value" column
# note that an input value is np.nan (null) to ensure the function properly handles null values
df = pd.DataFrame({"key":[0, 1, 2, 3], "value":[6, 36, 73, np.nan]})
 
# create unit test function
def test_add_one(df, column, expected_values):
    df = add_one(df, column)
    actual_values = list(df[column].values)
    assert actual_values == expected_values
 
# call the unit test function on sample inputs in the "value" column of df
test_add_one(df, "value", expected_values=[7, 37, 74, np.nan])
```
### Automate Unit Testing
The example above could either be run manually or automatically.
While manual testing is beneficial for initial, exploratory testing, it does not:
- Scale well as the application (and the number of related unit tests) grows
- Facilitate running unit tests frequently to prevent unnoticed degradation of data quality

This is where automated unit testing comes in. There are test runners in Python that streamline the process of
 automating unit tests. The most popular package is unittest, which comes pre-installed with the standard Python
 library. Other test runners (e.g., pytest, nose2) exist and are worth exploring, but are outside the scope of this
 documentation.

``` 
#-----------example of an automated unit test-----------#
# import libraries
import unittest
import pandas as pd
import numpy as np
 
# import function to be tested
from example_script import add_one
 
# create sample inputs in the "value" column
# note that an input value is np.nan (null) to ensure the function properly handles null values
df = pd.DataFrame({"key":[0, 1, 2, 3], "value":[6, 36, 73, np.nan]})
 
# create class that takes unittest.TestCase as an argument
# including unittest.TestCase as an argument lets unittest automate the unit test
class TestAddOne(unittest.TestCase):
    def test_add_one(self):
    """
    Test that the add_one function returns outputs as expected
    """
        df = add_one(df, "value")
        actual_values = list(df[column].values)
        expected_values = [7, 37, 74, np.nan]
 
        # call assertEqual method from unittest.TestCase
        self.assertEqual(actual_values, expected_values, "Should be {}".format(expected_values))
 
# 
if __name__ == '__main__':
    unittest.main()
```
### Where to Place Unit Tests
The example above places all relevant code in the same script for instructional purposes. However, it is common to
 separate unit tests from the code they are testing. We recommend creating a folder called "tests/" within the root
 directory of the project, so that the folder can easily access other directories in the project. Some large projects
 might benefit from adding subdirectories of tests within the "test/" folder based on their purpose or usage. Use your
 best judgment. Each file should start with the "test_" prefix, which Python test runner libraries recognize as unit
 test files.



## Debuggers and Data Preview
In many integrated development environments (IDEs), including Palantir’s Foundry or Visual Studio Code, there are
 debugging modes or views that allow the user to identify and troubleshoot potential bugs. This includes options to
 run/resume (or pause) code execution or to step over or step into method calls. There are also options to insert
 breakpoints, often by clicking to the left of the relevant line, where the debugger will temporarily pause execution on
 that line as it runs so you see what’s happening. There are other advanced debugging features depending on the tool,
 but these are the fundamentals that can help troubleshoot issues as you encounter them.



## PySpark Dataset Build Concepts and Considerations
### Dataset Quality Control Checks
When creating new datasets and data pipelines, ensure that written code executes properly and that the resulting
 dataset(s) are successfully created. There are several quality control checks that can be performed on built datasets
 that can increase confidence in the results.
- Row count check
    - Make sure that the number of rows in a dataset make sense. 
- Column count check
    - Check the name and number of columns in a dataset to make sure that nothing dropped off that should not have.
- Value range assumption checks
    - Individual column statistics can be calculated on numerical columns to make sure that the range of values in each
      column make sense.
    - Checks on non-numeric columns can also be performed, such as the number of unique values in a column. 
    
It is important to perform these checks and to ensure that PySpark code is executing and building a dataset properly.
 Faulty code can have a major impact on all downstream datasets and transformations, which can misrepresent data and
 invalidate results and insights derived from analysis performed on data.



## Incremental Builds
Traditional data transformation scripts recompute output datasets in their entirety each time they are run. This can
 become computationally expensive and wasteful, especially with very large datasets being transformed. Certain platforms
 and software allow for incremental data loads or transformations, which detect new records in an iteration of an input
 dataset and perform calculations only on those new records and not on the old records which have already gone through
 transformation. 

How incremental data loads are achieved depends upon the software being used. For example, Palantir's Foundry platform
 enables incremental data transforms through its *@incremental()* decorator, which allows for new records to either be
 appended on to an output dataset or for the output dataset to be fully replaced, depending on parameter settings in the
 code. Another example is Qlik Sense's Data Load Editor which allows for incremental data load scripts to be written to
 save on compute resources. 
