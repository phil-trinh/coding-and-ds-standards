# 3 - Business Logic
## Expected Outputs from this Stage
The primary outputs here are the DAG (a Directed Acyclic Graph, a graph that shows how data flows from table to table
 through your pipeline) and the step-by-step transformations that outline the flow of data through your data pipeline. 

- [ ] Data Model Documentation (aka DAG)
- [ ] Business Logic (Step by Step Transformations)
  - [ ] Column Descriptions
  - [ ] Data Expectations


## Data Model Documentation (aka DAG) 
"A DAG is not a universal descriptor of a pipeline when you zoom in, but it is a descriptor at the high level of
 execution; it describes the steps being taken â€” aka the pipeline." [medium.com](https://medium.com/hashmapinc/building-ml-pipelines-8e27344a42d2)

DAG stands for directed acyclic graph. This can be thought of as the "horizontal logic" of your business logic.
 This allows you to take the whole (sometimes large!) pipeline and break it up into smaller logical chunks. It is a
 simple diagram of inputs, outputs, and high-level data transformation/integration activities, e.g., data cleaning,
 dataset joins, deriving new features, aggregating, and other I/O actions. Each node should be a unique data
 transformation; and the edges connect those together in order. 
 
It is a called a DAG because it may have multiple input and output paths, but should not contain loops. The edges
 between each transformation step should be unidirectional arrows -- it is clear in which direction the data flows. It
 represents the _what_ of the transformations (e.g., 'convert dates to months'; and 'aggregate by month') but not the
 _how_ of implementing in code -- that'll come later. Note that each node represents a logical step, not a physical
 step. In code, you may combine one or more of these into functions that write out intermediate datasets, for example. 

Microsoft PowerPoint is an easy tool for creating your DAG using shapes, or there are other apps/software that you could
 explore if you're feeling adventurous. 

DAGs should be used in tandem with written business logic (described below), which will contain more details about the
 transformations happening at each node. 


## Business Logic
This is where you describe _in English_ what's going on at each step in your DAG. The business logic can be used to
 communicate your plan to both technical (teammates, project lead) and non-technical (functional owners, customers)
 audiences. It's easier for non-technical stakeholders to take a look at your business logic document than parse through
 your code when you are validating your understanding of the logic. If you can't communicate your logic in plain
 language, you won't be able to implement it in code, so this is a necessary first step before creating your data
 pipeline in code.

Your business logic should be step-by-step descriptions of the transformations that are outlined in the DAG created
 above. It should contain the _what_ and also any notes on _why_ (Why do we filter out soldiers with brown eyes? Why do
 we need to fill missing records?). Each of these steps can end up becoming comment blocks within your code once you do
 implement it. This still does not indicate the _how_ of implementing this logic - in fact, there may be one or two or
 more ways to implement this logic in the code. The _how_ will be worked out in the pseudocode next.

The business logic outlined at this step does not contain specific column name references and values - those details
 will come in the pseudocode (e.g., it's OK to say "filter for class 9 parts" instead of "filter dataframe for
 'class_of_supply' == 9").

You should document the business logic in a consistent place near your code so that it is easy for future developers to
 find. Accessibility and version control are important here. You want this to be easy for future developers to find and
 update. Good possible tools for this are Confluence, Reports in Palantir Foundry, README markdown files in your code
 repository, or worst case, a Word document in Microsoft Teams/SharePoint to store this document. Whatever method you
 use to document your business logic, make sure it is a living document -- the business logic may change as you learn
 more from the customers, or iterate on the logic in the code. Both code and documentation should be updated together if
 logic changes. The documentation you write in this section will become the starting point for the pseudocode and
 comments you write in the next sections.

## Additional References
In this step, you'll also start planning for column descriptions, data expectations, and testing. Implementing all of
 these will make it easier to develop and maintain your data pipeline in the future. While you will ultimately be
 implementing these in code, just like the business logic, you can also plan these out in plain language to make it
 easier to refine and validate before writing code.

### Column Descriptions
You've already outlined your target schema, but what does it mean? Prepare short descriptions (< 100 words) of what the
 column represents, plus relevant context you've learned about the business logic, functional knowledge, format
 expectations, and the original source. You should update this over time, just like the business logic, if your code
 changes. 

Place this documentation alongside your target schema in your repo or within the dataframe itself, if you are using
 Palantir Foundry's column description capabilities. Different tools may have functionality for saving metadata
 alongside/within dataframe objects, but this can come later in the implementation phase.

Make sure to **update these** as is appropriate throughout the code development process -- column descriptions should be
 audited every time you write out a new dataset or update code.

### Data Expectations
Changes in your input dataset(s) due to things like data drift, system changes, or upstream logic changes may cause the
 contents of the data you rely on to change, affecting the accuracy of your analysis or models. This may happen both in
 input datasets but also within your own code -- as you write/update code in your pipelines, you may inadvertently
 introduce errors in the data, and this will catch them early.

At this stage, you should prepare a list of data expectations that you can logically and programmatically check in your
 data pipeline to ensure that the data continues to match assumptions and doesn't corrupt downstream datasets and
 analyses. A column may have 0, 1, or more expectations/assumptions. As you build out your target schema/data
 dictionary, write your expectations for each column (in English!) in a version controlled document that you can
 implement in code later.

Common types of data expectations:
- primary key column (on the input and/or the output) (every time you make a join you are implicitly making an
  assumption about primary keys, make these expectations explicit through your expectations, this way you won't end up
  with a row explosion if an ID is used multiple times in the future.)
- allowable values (numerical values between certain ranges or with a certain standard deviation; string values of
  certain lengths, valid characters/formats, or values from a closed list; dates of a certain format; arrays containing
  allowed values or equal to a certain length; etc.)
- relationships between columns (dates before/after each other, etc.)
- thresholds for null percentage/null counts 
- dates relative to current date (e.g., should not contain dates in the future)
- column existence (if your analysis relies on a column, make sure it exists in the input! You can either do this via a
  data expectation or by explicitly calling the column name in a select statement. If you call a column by name in your
  initial select you'll get a clear error if it fails, this is better than select * where columns could be added or
  deleted and your code will change behavior without you knowing it.)

Expectations may also be conditional on other expectations.

Attempt to use data expecations to "clean" raw data where you can by finding root cause. 
- Are there too many/few parsers in a row of data 
- Is one row of data being split into multiple rows?

### Test-Driven Development
All requirements, business logic, and data expectations should be converted to tests before implementing code. Borrowing
 from software engineering best practices, test-driven development has many benefits (stolen from [Mrigank Shekhar on
 Medium](https://medium.com/miq-tech-and-analytics/test-driven-development-in-data-science-190f1247ebbc)):
- Flexibility: with the code base growing, and multiple people working on multiple features, test cases ensure that the
  old code continues to work as new code is being added. Developer A doesn't have to worry that their code breaks the
  code developed by developer B as A can run tests to check that everything is working fine. 
- Documentation: because unit tests are isolated in nature, they help us understand what the code intends to do in a
  simple manner. So if A faces some issues because of a feature developed by B, it's convenient to look at the test
  cases, their input and output data, and the edge cases being handled
- Minimal debugging: as we are verifying the code throughout the process. Therefore, there will be very few bugs
  remaining after the feature has been developed.
- Better design of the code: breaking the feature into small isolated pieces of code organically leads to a better and
  simpler implementation
- Edge cases: writing tests forces the developer to think of cases which may cause failures and need to be handles
- Data processing: the preprocessing steps in data science get all the benefits that TTD provides. For e.g., we can
  check that steps such as filtering, removing columns/rows with a large number of missing values, feature engineering,
  etc. are working
- Working in teams: a data scientist may not be the only one working on a modeling project. So structuring code, writing
  simple asserts, unit tests, etc. help teams
- Building a pipeline: in most companies, a data scientist is not only involved in the modeling of data. They may also
  work in building features using the results of modeling or analysis of data.

Data expectations save you from developing or changing code that ends up violating a business rule or data expectations
 that you have already identified above. 

There are a few python libraries that make creating and monitoring tests easier, we recommend pytest.

Each step in the business logic is a distinct unit that can/should have a test written for it, which will allow you to
 easily compartmentalize operations and easily debug code. These tests may be related to the data expectations
 identified above, or may test different aspects of your data pipeline. 

At this stage, we are not implementing the test in the code, but when we outline the business logic, each step should
 have an example test (or more!) written alongside it. For example, if your business logic step is to filter out certain
 values, identify a few examples of values you would and wouldn't expect to remain after the filtering. And make sure
 you consider edge cases! In our filtering example: if you want to filter out rows where a column has values of "UNITED
 STATES," depending on your faith in the quality of the data, you may want to check that you correctly filter out
 "United States " and "UNITED STATES ". Also, make sure you think about how you want to handle "NULL" values and
 explicitly include them in your tests.
